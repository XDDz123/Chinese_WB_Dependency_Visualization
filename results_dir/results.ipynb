{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "719d99af",
   "metadata": {},
   "source": [
    "### parsing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7883412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "import stanza\n",
    "\n",
    "with open(\"zh_gsdsimp-ud-test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "   raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da92e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:47:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b7d55b017440e88450b5db3eb87d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:47:07 INFO: Downloaded file to /Users/jungyeul/stanza_resources/resources.json\n",
      "2025-03-28 20:47:09 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | models/UD_...kenizer.pt |\n",
      "| pos       | models/UD_..._tagger.pt |\n",
      "| lemma     | gsdsimp_nocharlm        |\n",
      "| depparse  | models/UD_..._parser.pt |\n",
      "=======================================\n",
      "\n",
      "2025-03-28 20:47:09 INFO: Using device: cpu\n",
      "2025-03-28 20:47:09 INFO: Loading: tokenize\n",
      "2025-03-28 20:47:09 INFO: Loading: pos\n",
      "2025-03-28 20:47:10 INFO: Loading: lemma\n",
      "2025-03-28 20:47:10 INFO: Loading: depparse\n",
      "2025-03-28 20:47:12 INFO: Done loading processors!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     91.45 |     93.55 |     92.49 |\n",
      "Sentences  |     99.40 |     98.80 |     99.10 |\n",
      "Words      |     91.45 |     93.55 |     92.49 |\n",
      "UPOS       |     86.29 |     88.27 |     87.27 |     94.36\n",
      "XPOS       |     86.85 |     88.84 |     87.83 |     94.97\n",
      "UFeats     |     79.67 |     81.50 |     80.57 |     87.12\n",
      "AllTags    |     74.34 |     76.04 |     75.18 |     81.28\n",
      "Lemmas     |     82.69 |     84.59 |     83.63 |     90.42\n",
      "UAS        |     75.28 |     77.01 |     76.13 |     82.31\n",
      "LAS        |     72.06 |     73.72 |     72.88 |     78.80\n",
      "CLAS       |     68.50 |     70.42 |     69.45 |     77.37\n",
      "MLAS       |     51.05 |     52.49 |     51.76 |     57.67\n",
      "BLEX       |     57.83 |     59.46 |     58.64 |     65.32\n"
     ]
    }
   ],
   "source": [
    "nlp_LTP = stanza.Pipeline(\n",
    "    lang=\"zh-hans\",\n",
    "    depparse_model_path=\"models/UD_Chinese-GSDLTP/UD_Chinese-GSDSimp_model-bert/saved_models/depparse/zh-hans_gsdsimp_electra-large_parser.pt\",\n",
    "    pos_model_path=\"models/UD_Chinese-GSDLTP/UD_Chinese-GSDSimp_model-bert/saved_models/pos/zh-hans_gsdsimp_charlm_tagger.pt\",\n",
    "    tokenize_model_path=\"models/UD_Chinese-GSDLTP/UD_Chinese-GSDSimp_model-bert/saved_models/tokenize/zh-hans_gsdsimp_tokenizer.pt\",\n",
    "    processors='tokenize,pos,lemma,depparse', )\n",
    "\n",
    "doc = nlp_LTP(raw_text)\n",
    "\n",
    "with open(\"test-ltp-prediction.conllu\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for sent_id, sent in enumerate(doc.sentences, start=1):\n",
    "        out_file.write(f\"# sent_id = {sent_id}\\n\")\n",
    "        out_file.write(f\"# text = {' '.join([w.text for w in sent.words])}\\n\")\n",
    "        for i, word in enumerate(sent.words, start=1):\n",
    "            out_file.write(\n",
    "                f\"{i}\\t{word.text}\\t{word.lemma}\\t{word.upos}\\t{word.xpos}\\t_\\t\"\n",
    "                f\"{word.head}\\t{word.deprel}\\t_\\t_\\n\"\n",
    "            )\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "import sys\n",
    "!{sys.executable} conll18_ud_eval.py -v  test-ltp-gold.conllu test-ltp-prediction.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e07ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:48:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5583f841e81644b9ab52b759f50becbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:48:16 INFO: Downloaded file to /Users/jungyeul/stanza_resources/resources.json\n",
      "2025-03-28 20:48:17 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | models/UD_...kenizer.pt |\n",
      "| pos       | models/UD_..._tagger.pt |\n",
      "| lemma     | gsdsimp_nocharlm        |\n",
      "| depparse  | models/UD_..._parser.pt |\n",
      "=======================================\n",
      "\n",
      "2025-03-28 20:48:17 INFO: Using device: cpu\n",
      "2025-03-28 20:48:17 INFO: Loading: tokenize\n",
      "2025-03-28 20:48:18 INFO: Loading: pos\n",
      "2025-03-28 20:48:19 INFO: Loading: lemma\n",
      "2025-03-28 20:48:19 INFO: Loading: depparse\n",
      "2025-03-28 20:48:20 INFO: Done loading processors!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     90.46 |     92.53 |     91.48 |\n",
      "Sentences  |     99.40 |     98.80 |     99.10 |\n",
      "Words      |     90.46 |     92.53 |     91.48 |\n",
      "UPOS       |     84.65 |     86.58 |     85.60 |     93.57\n",
      "XPOS       |     85.91 |     87.88 |     86.89 |     94.97\n",
      "UFeats     |     78.84 |     80.64 |     79.73 |     87.15\n",
      "AllTags    |     73.19 |     74.86 |     74.02 |     80.91\n",
      "Lemmas     |     83.20 |     85.11 |     84.14 |     91.98\n",
      "UAS        |     73.68 |     75.36 |     74.51 |     81.44\n",
      "LAS        |     70.59 |     72.20 |     71.38 |     78.03\n",
      "CLAS       |     66.39 |     68.40 |     67.38 |     76.38\n",
      "MLAS       |     49.29 |     50.78 |     50.02 |     56.70\n",
      "BLEX       |     57.99 |     59.74 |     58.85 |     66.71\n"
     ]
    }
   ],
   "source": [
    "nlp_Penn = stanza.Pipeline(\n",
    "    lang=\"zh-hans\",\n",
    "    depparse_model_path=\"models/UD_Chinese-GSDPenn/UD_Chinese-GSDSimp_model-bert/saved_models/depparse/zh-hans_gsdsimp_electra-large_parser.pt\",\n",
    "    pos_model_path=\"models/UD_Chinese-GSDPenn/UD_Chinese-GSDSimp_model-bert/saved_models/pos/zh-hans_gsdsimp_charlm_tagger.pt\",\n",
    "    tokenize_model_path=\"models/UD_Chinese-GSDPenn/UD_Chinese-GSDSimp_model-bert/saved_models/tokenize/zh-hans_gsdsimp_tokenizer.pt\",\n",
    "    processors='tokenize,pos,lemma,depparse', )\n",
    "\n",
    "doc = nlp_Penn(raw_text)\n",
    "\n",
    "with open(\"test-penn-prediction.conllu\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for sent_id, sent in enumerate(doc.sentences, start=1):\n",
    "        out_file.write(f\"# sent_id = {sent_id}\\n\")\n",
    "        out_file.write(f\"# text = {' '.join([w.text for w in sent.words])}\\n\")\n",
    "        for i, word in enumerate(sent.words, start=1):\n",
    "            out_file.write(\n",
    "                f\"{i}\\t{word.text}\\t{word.lemma}\\t{word.upos}\\t{word.xpos}\\t_\\t\"\n",
    "                f\"{word.head}\\t{word.deprel}\\t_\\t_\\n\"\n",
    "            )\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "import sys\n",
    "!{sys.executable} conll18_ud_eval.py -v  test-penn-gold.conllu test-penn-prediction.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbba0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:49:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ea58d1b7b04d04b27fc3448c6df085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:49:24 INFO: Downloaded file to /Users/jungyeul/stanza_resources/resources.json\n",
      "2025-03-28 20:49:25 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | models/UD_...kenizer.pt |\n",
      "| pos       | models/UD_..._tagger.pt |\n",
      "| lemma     | gsdsimp_nocharlm        |\n",
      "| depparse  | models/UD_..._parser.pt |\n",
      "=======================================\n",
      "\n",
      "2025-03-28 20:49:25 INFO: Using device: cpu\n",
      "2025-03-28 20:49:25 INFO: Loading: tokenize\n",
      "2025-03-28 20:49:25 INFO: Loading: pos\n",
      "2025-03-28 20:49:27 INFO: Loading: lemma\n",
      "2025-03-28 20:49:27 INFO: Loading: depparse\n",
      "2025-03-28 20:49:29 INFO: Done loading processors!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     89.33 |     91.54 |     90.42 |\n",
      "Sentences  |     98.80 |     98.80 |     98.80 |\n",
      "Words      |     89.33 |     91.54 |     90.42 |\n",
      "UPOS       |     83.41 |     85.48 |     84.44 |     93.38\n",
      "XPOS       |     85.02 |     87.13 |     86.06 |     95.18\n",
      "UFeats     |     77.83 |     79.76 |     78.78 |     87.13\n",
      "AllTags    |     72.42 |     74.21 |     73.31 |     81.07\n",
      "Lemmas     |     82.29 |     84.33 |     83.30 |     92.13\n",
      "UAS        |     72.32 |     74.11 |     73.21 |     80.96\n",
      "LAS        |     69.32 |     71.04 |     70.17 |     77.60\n",
      "CLAS       |     65.17 |     67.57 |     66.35 |     76.59\n",
      "MLAS       |     48.22 |     49.99 |     49.09 |     56.67\n",
      "BLEX       |     57.23 |     59.33 |     58.26 |     67.25\n"
     ]
    }
   ],
   "source": [
    "nlp_PKU = stanza.Pipeline(\n",
    "    lang=\"zh-hans\",\n",
    "    depparse_model_path=\"models/UD_Chinese-GSDPKU/UD_Chinese-GSDSimp_model-bert/saved_models/depparse/zh-hans_gsdsimp_electra-large_parser.pt\",\n",
    "    pos_model_path=\"models/UD_Chinese-GSDPKU/UD_Chinese-GSDSimp_model-bert/saved_models/pos/zh-hans_gsdsimp_charlm_tagger.pt\",\n",
    "    tokenize_model_path=\"models/UD_Chinese-GSDPKU/UD_Chinese-GSDSimp_model-bert/saved_models/tokenize/zh-hans_gsdsimp_tokenizer.pt\",\n",
    "    processors='tokenize,pos,lemma,depparse', )\n",
    "    \n",
    "doc = nlp_PKU(raw_text)\n",
    "\n",
    "with open(\"test-pku-prediction.conllu\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for sent_id, sent in enumerate(doc.sentences, start=1):\n",
    "        out_file.write(f\"# sent_id = {sent_id}\\n\")\n",
    "        out_file.write(f\"# text = {' '.join([w.text for w in sent.words])}\\n\")\n",
    "        for i, word in enumerate(sent.words, start=1):\n",
    "            out_file.write(\n",
    "                f\"{i}\\t{word.text}\\t{word.lemma}\\t{word.upos}\\t{word.xpos}\\t_\\t\"\n",
    "                f\"{word.head}\\t{word.deprel}\\t_\\t_\\n\"\n",
    "            )\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "import sys\n",
    "!{sys.executable} conll18_ud_eval.py -v  test-pku-gold.conllu test-pku-prediction.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013695b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:51:31 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6d99bac61c428dba61507abfe35865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:51:31 INFO: Downloaded file to /Users/jungyeul/stanza_resources/resources.json\n",
      "2025-03-28 20:51:32 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | models/UD_...kenizer.pt |\n",
      "| pos       | models/UD_..._tagger.pt |\n",
      "| lemma     | gsdsimp_nocharlm        |\n",
      "| depparse  | models/UD_..._parser.pt |\n",
      "=======================================\n",
      "\n",
      "2025-03-28 20:51:32 INFO: Using device: cpu\n",
      "2025-03-28 20:51:32 INFO: Loading: tokenize\n",
      "2025-03-28 20:51:32 INFO: Loading: pos\n",
      "2025-03-28 20:51:33 INFO: Loading: lemma\n",
      "2025-03-28 20:51:34 INFO: Loading: depparse\n",
      "2025-03-28 20:51:35 INFO: Done loading processors!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     93.47 |     94.51 |     93.99 |\n",
      "Sentences  |     99.40 |     98.80 |     99.10 |\n",
      "Words      |     93.47 |     94.51 |     93.99 |\n",
      "UPOS       |     90.05 |     91.04 |     90.54 |     96.34\n",
      "XPOS       |     89.87 |     90.87 |     90.37 |     96.15\n",
      "UFeats     |     82.73 |     83.64 |     83.18 |     88.50\n",
      "AllTags    |     79.06 |     79.94 |     79.50 |     84.58\n",
      "Lemmas     |     92.97 |     94.00 |     93.48 |     99.46\n",
      "UAS        |     79.61 |     80.49 |     80.05 |     85.17\n",
      "LAS        |     76.69 |     77.54 |     77.11 |     82.05\n",
      "CLAS       |     74.29 |     75.29 |     74.79 |     81.43\n",
      "MLAS       |     57.87 |     58.65 |     58.26 |     63.44\n",
      "BLEX       |     74.01 |     75.01 |     74.51 |     81.13\n"
     ]
    }
   ],
   "source": [
    "nlp_GSD = stanza.Pipeline(\n",
    "    lang=\"zh-hans\",\n",
    "    # download_method=DownloadMethod.REUSE_RESOURCES,\n",
    "    depparse_model_path=\"models/UD_Chinese-GSDSimp/UD_Chinese-GSDSimp_model-bert/saved_models/depparse/zh-hans_gsdsimp_electra-large_parser.pt\",\n",
    "    pos_model_path=\"models/UD_Chinese-GSDSimp/UD_Chinese-GSDSimp_model-bert/saved_models/pos/zh-hans_gsdsimp_charlm_tagger.pt\",\n",
    "    tokenize_model_path=\"models/UD_Chinese-GSDSimp/UD_Chinese-GSDSimp_model-bert/saved_models/tokenize/zh-hans_gsdsimp_tokenizer.pt\",\n",
    "    processors='tokenize,pos,lemma,depparse', ) \n",
    "\n",
    "doc = nlp_GSD(raw_text)\n",
    "\n",
    "with open(\"test-gsd-prediction.conllu\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for sent_id, sent in enumerate(doc.sentences, start=1):\n",
    "        out_file.write(f\"# sent_id = {sent_id}\\n\")\n",
    "        out_file.write(f\"# text = {' '.join([w.text for w in sent.words])}\\n\")\n",
    "        for i, word in enumerate(sent.words, start=1):\n",
    "            out_file.write(\n",
    "                f\"{i}\\t{word.text}\\t{word.lemma}\\t{word.upos}\\t{word.xpos}\\t_\\t\"\n",
    "                f\"{word.head}\\t{word.deprel}\\t_\\t_\\n\"\n",
    "            )\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "import sys\n",
    "!{sys.executable} conll18_ud_eval.py -v  test-gsd-gold.conllu test-gsd-prediction.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0ed40",
   "metadata": {},
   "source": [
    "### segmentation reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4c6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jp_algorithm import load_txt_file\n",
    "from jp_algorithm import evaluate as txt_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5a5998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\tgsd\n",
      "F score =  0.9398518027900815\n",
      "-----\n",
      "\n",
      "Corpus:\tltp\n",
      "F score =  0.9251036388760939\n",
      "-----\n",
      "\n",
      "Corpus:\tpenn\n",
      "F score =  0.8903423699760926\n",
      "-----\n",
      "\n",
      "Corpus:\tpku\n",
      "F score =  0.8689556637049606\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_nothing():\n",
    "    return  0\n",
    "\n",
    "corpus = ['gsd', 'ltp', 'penn', 'pku']\n",
    "\n",
    "gold = 'gold'\n",
    "predict = 'prediction'\n",
    "\n",
    "for i in corpus:\n",
    "\n",
    "    print('Corpus:\\t' + i)\n",
    "\n",
    "    conllu = \"test-\" + i + \"-\" + predict + \".conllu\"\n",
    "\n",
    "    with open (conllu, \"r\") as file_in:\n",
    "        data = file_in.read().splitlines()\n",
    "\n",
    "    predict_file = \"test-\" + i + \"-\" + predict + \".conllu.wb.txt\"\n",
    "    f = open(predict_file, \"w\")\n",
    "\n",
    "    sentence = []\n",
    "\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        if line.startswith('#'):\n",
    "            do_nothing()\n",
    "        elif len(line) == 0:\n",
    "            do_nothing()\n",
    "            if len(sentence) > 0:\n",
    "                f.write(' '.join(sentence) + '\\n')\n",
    "            sentence = []\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            sentence.append(line[1])\n",
    "    f.close()\n",
    "    \n",
    "    gold_file = \"test-\" + i + \"-\" + gold + \".conllu.wb.txt\"\n",
    "\n",
    "\n",
    "    gold_txt = load_txt_file(gold_file)\n",
    "    sys_txt  = load_txt_file(predict_file)\n",
    "\n",
    "    evaluation = txt_evaluate(gold_txt, sys_txt)\n",
    "    for metric in[\"Tokens\"]:\n",
    "        # print(\"{:11}|{:10} & {:10} & {:10} | {:10}\".format(\n",
    "        #     metric,\n",
    "        #     evaluation[metric].correct,\n",
    "        #     evaluation[metric].system_total - evaluation[metric].correct,\n",
    "        #     evaluation[metric].gold_total - evaluation[metric].correct,\n",
    "        #     evaluation[metric].aligned_total or (evaluation[metric].correct if metric == \"Words\" else \"\")\n",
    "        # ))\n",
    "\n",
    "        precision = evaluation[metric].correct / evaluation[metric].system_total \n",
    "        recall = evaluation[metric].correct / evaluation[metric].gold_total \n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "        print('F score = ', f_score)\n",
    "\n",
    "    print('-----\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
